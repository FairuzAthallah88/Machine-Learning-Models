{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FairuzAthallah88/Machine-Learning-Models/blob/main/CNN_Xplorin_MobileNetV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3DtLimbtrY-"
      },
      "source": [
        "# MODEL CNN XPLORIN MobileNetV3 (1 Fase Training)\n",
        "\n",
        "- Arsitektur\t: MobileNetV3\n",
        "- IMG HEIGHT & IMG WIDTH : $224 \\times 224$\n",
        "- BATCH SIZE\t: 32\n",
        "- LEARNING RATE : LR_MAX $0.00005$ ; LR_MIN $0.0000001$\n",
        "- EPOCH : 100\n",
        "- DATASET : https://drive.google.com/drive/folders/1g4rL66pXysnmAFzhcyOV6Fl9K3O_cPmG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLzBZPGyAwp"
      },
      "source": [
        "1. Instalasi, Imports, dan Konfigurasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wpBT7kwNvP2W"
      },
      "outputs": [],
      "source": [
        "# --- Imports Library ---\n",
        "import joblib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.applications import MobileNetV3Large\n",
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DATASET_MENTAH_DIR = '/content/drive/MyDrive/DATASET_CNN_XPLORIN/Xplorin'\n",
        "ASSET_SAVE_DIR = '/content/drive/MyDrive/DATASET_CNN_XPLORIN/MODEL_ASSETS'\n",
        "os.makedirs(ASSET_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# File Model V3\n",
        "MODEL_SAVE_PATH = os.path.join(ASSET_SAVE_DIR, 'best_mobilenet_v3_large.h5')\n",
        "\n",
        "# Folder data\n",
        "BASE_SPLIT_DIR = os.path.join(DATASET_MENTAH_DIR, 'dataset_split')\n",
        "TRAINING_DIR = os.path.join(BASE_SPLIT_DIR, 'training')\n",
        "VALIDATION_DIR = os.path.join(BASE_SPLIT_DIR, 'validation')\n",
        "\n",
        "# HYPERPARAMETER\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "TOTAL_EPOCHS = 100\n",
        "\n",
        "# Learning Rate\n",
        "LR_MAX = 5e-5\n",
        "LR_MIN = 1e-7\n",
        "\n",
        "# Callback\n",
        "PATIENCE_EARLY_STOP = 20\n",
        "PATIENCE_REDUCE_LR = 10\n",
        "\n",
        "print(f\"Target Penyimpanan Model: {MODEL_SAVE_PATH}\")\n",
        "print(\"Konfigurasi MobileNetV3Large Selesai.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5iAfPYEBRp5"
      },
      "source": [
        "2. Visualisasi Sampel Dataset Mentah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EJsNT8_bA9Yn"
      },
      "outputs": [],
      "source": [
        "# --- Visualisasi Sampel Dataset Mentah ---\n",
        "all_categories = [d for d in os.listdir(DATASET_MENTAH_DIR) if os.path.isdir(os.path.join(DATASET_MENTAH_DIR, d)) and d not in ['dataset_split', '.ipynb_checkpoints']]\n",
        "\n",
        "if not all_categories:\n",
        "    print(\"Gagal menemukan folder kategori di DATASET_MENTAH_DIR. Periksa kembali path.\")\n",
        "else:\n",
        "    # Ambil 9 kategori secara acak\n",
        "    num_to_display = min(9, len(all_categories))\n",
        "\n",
        "    # Pilih kategori secara acak\n",
        "    categories_to_display = random.sample(all_categories, num_to_display)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.suptitle(\"Sampel Gambar Dataset Mentah (Sebelum Preprocessing)\", fontsize=16)\n",
        "\n",
        "    # Tampilkan satu gambar per kategori\n",
        "    for i, category in enumerate(categories_to_display):\n",
        "        category_path = os.path.join(DATASET_MENTAH_DIR, category)\n",
        "\n",
        "        # Ambil semua file gambar dalam kategori tersebut\n",
        "        image_files = [f for f in os.listdir(category_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if image_files:\n",
        "            # Pilih satu gambar secara acak\n",
        "            random_image_file = random.choice(image_files)\n",
        "            image_path = os.path.join(category_path, random_image_file)\n",
        "\n",
        "            # Tampilkan gambar\n",
        "            plt.subplot(3, 3, i + 1)\n",
        "            img = Image.open(image_path)\n",
        "            plt.imshow(img)\n",
        "            plt.title(category, fontsize=12)\n",
        "            plt.axis('off')\n",
        "        else:\n",
        "            plt.subplot(3, 3, i + 1)\n",
        "            plt.title(f\"{category} (Kosong)\", fontsize=12)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "    print(f\" Visualisasi Sampel dari {num_to_display} kategori selesai.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4naSDVi_xZu"
      },
      "source": [
        "3. Data Augmentation dan Generator (Prepeocessing Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9h0QFom1_v0-"
      },
      "outputs": [],
      "source": [
        "# --- Data Augmentation & Generator MobileNetV3 ---\n",
        "# 1. Generator TRAINING\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# 2. Generator VALIDATION\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "# 3. Pemuatan Data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "NUM_CLASSES = train_generator.num_classes\n",
        "CLASS_NAMES = list(train_generator.class_indices.keys())\n",
        "\n",
        "print(f\"\\nData Generators V3 Siap. Total Kelas: {NUM_CLASSES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsUweyzrGEbm"
      },
      "source": [
        "4. Arsitektur Model MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aotOgWw9GFqP"
      },
      "outputs": [],
      "source": [
        "# --- Arsitektur Model MobileNetV3Large ---\n",
        "print(\"--- DEFINISI ARSITEKTUR MODEL (MobileNetV3Large) ---\")\n",
        "\n",
        "# Gunakan MobileNetV3Large\n",
        "base_model = MobileNetV3Large(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
        ")\n",
        "\n",
        "# 1. Unfreeze Base Model\n",
        "base_model.trainable = True\n",
        "print(f\"Jumlah Layer Base Model MobileNetV3Large: {len(base_model.layers)}\")\n",
        "\n",
        "# 2. Bangun Lapisan Klasifikasi (Head)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Lapisan Dense dan Dropout\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.6)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.6)(x)\n",
        "\n",
        "# Lapisan Output\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# Model Akhir\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "print(\"\\nArsitektur Model MobileNetV3Large Siap.\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6XFXsFcID8m"
      },
      "source": [
        "5. Proses Training Terpadu (Hyperparameter Tuning: Penyesuaian LR & Regularisasi)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beAtCtEA8b16",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# --- Proses Training Terpadu ---\n",
        "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
        "total_steps = TOTAL_EPOCHS * steps_per_epoch\n",
        "\n",
        "# Scheduler\n",
        "lr_schedule = CosineDecay(\n",
        "    initial_learning_rate=LR_MAX,\n",
        "    decay_steps=total_steps,\n",
        "    alpha=LR_MIN/LR_MAX\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "callbacks_list_final = [\n",
        "    EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOP, restore_best_weights=True),\n",
        "    ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True),\n",
        "]\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- MEMULAI TRAINING MOBILE NET V3 ---\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=TOTAL_EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    callbacks=callbacks_list_final,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training V3 Selesai.\")\n",
        "\n",
        "HISTORY_SAVE_PATH = os.path.join(ASSET_SAVE_DIR, 'training_history_v3_large.pkl')\n",
        "try:\n",
        "    joblib.dump(history.history, HISTORY_SAVE_PATH)\n",
        "    print(f\"‚úÖ Riwayat training disimpan ke: {HISTORY_SAVE_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Gagal menyimpan riwayat: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy5-vhIxPpu1"
      },
      "source": [
        "6. Visualisasi History & Evaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAACX0cIPqDT"
      },
      "outputs": [],
      "source": [
        "# --- Visualisasi History & Evaluasi Model ---\n",
        "HISTORY_PATH = os.path.join(ASSET_SAVE_DIR, 'training_history_v3_large.pkl')\n",
        "MODEL_PATH = os.path.join(ASSET_SAVE_DIR, 'best_mobilenet_v3_large.h5')\n",
        "\n",
        "# 1. PLOT GRAFIK HISTORY\n",
        "try:\n",
        "    print(f\" Memuat history dari: {HISTORY_PATH}\")\n",
        "    history_data = joblib.load(HISTORY_PATH)\n",
        "\n",
        "    hist_acc = history_data['accuracy']\n",
        "    hist_val_acc = history_data['val_accuracy']\n",
        "    hist_loss = history_data['loss']\n",
        "    hist_val_loss = history_data['val_loss']\n",
        "    epochs = range(1, len(hist_acc) + 1)\n",
        "    random_guess = 1 / NUM_CLASSES\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Grafik LOSS\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, hist_loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochs, hist_val_loss, 'r', label='Validation Loss')\n",
        "    plt.title('Loss: MobileNetV3Large')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Grafik ACCURACY\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, hist_acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochs, hist_val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.axhline(y=random_guess, color='green', linestyle='--', label=f'Random ({random_guess*100:.1f}%)')\n",
        "    plt.title('Accuracy: MobileNetV3Large')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning Grafik: {e} (Mungkin file history belum ada)\")\n",
        "\n",
        "# 2. LOAD WEIGHTS & EVALUASI\n",
        "print(f\"\\n Memuat bobot terbaik dari: {MODEL_PATH}\")\n",
        "try:\n",
        "    model.load_weights(MODEL_PATH)\n",
        "    print(\"‚úÖ Bobot berhasil dimuat!\")\n",
        "\n",
        "    # COMPILE MODEL\n",
        "    print(\"Compile model untuk evaluasi...\")\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Evaluasi\n",
        "    if 'validation_generator' in globals():\n",
        "        print(\"\\n--- MENJALANKAN EVALUASI PADA VALIDATION SET ---\")\n",
        "        validation_generator.reset()\n",
        "        results = model.evaluate(validation_generator, steps=validation_generator.samples // BATCH_SIZE)\n",
        "        print(\"\\n==============================================\")\n",
        "        print(f\" HASIL AKHIR MOBILENET V3 LARGE\")\n",
        "        print(f\" Loss Akhir:     {results[0]:.4f}\")\n",
        "        print(f\" Accuracy Akhir: {results[1]*100:.2f}%\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Generator tidak ditemukan.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error Evaluasi: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R2MvnZ6SnDL"
      },
      "source": [
        "7. Classification Report (Precision, Recall, F1-Score) dan Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb049nqwStLZ"
      },
      "outputs": [],
      "source": [
        "# --- Classification Report & Confusion Matrix ---\n",
        "try:\n",
        "    validation_generator.reset()\n",
        "    true_labels = validation_generator.classes\n",
        "    class_names = list(validation_generator.class_indices.keys())\n",
        "\n",
        "    print(\"--- MEMULAI PREDIKSI ---\")\n",
        "    predictions = model.predict(\n",
        "        validation_generator,\n",
        "        steps=validation_generator.samples // validation_generator.batch_size + 1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    predictions = predictions[:len(true_labels)]\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # 1. Confusion Matrix\n",
        "    print(\"\\n==============================================\")\n",
        "    print(\"             CONFUSION MATRIX\")\n",
        "    print(\"==============================================\")\n",
        "    print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "    # 2. Classification Report\n",
        "    print(\"\\n==============================================\")\n",
        "    print(\"           CLASSIFICATION REPORT\")\n",
        "    print(\"==============================================\")\n",
        "    print(classification_report(true_labels, predicted_labels, target_names=class_names, digits=4))\n",
        "    print(\"==============================================\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ùå Error: Variabel 'model' atau 'validation_generator' tidak ditemukan.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Terjadi kesalahan: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LoLuYhi-PJw"
      },
      "source": [
        "# OPSIONAL!!\n",
        "---\n",
        "UNTUK SPLIT DATASET DAN COMPARE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMc_qm7EbUn6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "DATASET_MENTAH_DIR = '/content/drive/MyDrive/DATASET_CNN_XPLORIN/Xplorin'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kr2nVykOX9hZ"
      },
      "outputs": [],
      "source": [
        "# --- SPLIT DATASET MENTAH MENJADI TRAIN & VAL ---\n",
        "\n",
        "IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.JPG', '.JPEG', '.PNG')\n",
        "print(\"\\n--- Otomasi Pembagian Dataset Dimulai ---\")\n",
        "split_results = defaultdict(lambda: {'mentah': 0, 'train_sukses': 0, 'val_sukses': 0, 'gagal_copy': 0})\n",
        "\n",
        "try:\n",
        "    # 1. Pembersihan dan Pembuatan Folder Baru\n",
        "    if os.path.exists(BASE_SPLIT_DIR):\n",
        "        print(\"Membersihkan folder 'dataset_split' yang lama...\")\n",
        "        shutil.rmtree(BASE_SPLIT_DIR)\n",
        "\n",
        "    os.makedirs(TRAINING_DIR, exist_ok=True)\n",
        "    os.makedirs(VALIDATION_DIR, exist_ok=True)\n",
        "\n",
        "    total_files_moved = 0\n",
        "\n",
        "    all_categories = [d for d in os.listdir(DATASET_MENTAH_DIR) if os.path.isdir(os.path.join(DATASET_MENTAH_DIR, d)) and d not in ['dataset_split', '.ipynb_checkpoints']]\n",
        "\n",
        "    if not all_categories:\n",
        "        print(\"‚ö†Ô∏è Gagal: Tidak ada folder kategori yang ditemukan. Periksa DATASET_MENTAH_DIR.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 2. Proses Pembagian Per Kategori\n",
        "    for class_name in all_categories:\n",
        "        class_path = os.path.join(DATASET_MENTAH_DIR, class_name)\n",
        "\n",
        "        # Membuat struktur folder\n",
        "        os.makedirs(os.path.join(TRAINING_DIR, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(VALIDATION_DIR, class_name), exist_ok=True)\n",
        "\n",
        "        images = [os.path.join(class_path, f) for f in os.listdir(class_path) if f.endswith(IMAGE_EXTENSIONS)]\n",
        "\n",
        "        split_results[class_name]['mentah'] = len(images)\n",
        "\n",
        "        random.shuffle(images)\n",
        "\n",
        "        if not images:\n",
        "            continue\n",
        "\n",
        "        # Hitung batas pembagian 80:20\n",
        "        train_split_point = int(len(images) * SPLIT_RATIO)\n",
        "        train_images = images[:train_split_point]\n",
        "        val_images = images[train_split_point:]\n",
        "\n",
        "        # Menyalin gambar ke folder Training\n",
        "        for img_path in tqdm(train_images, desc=f\"TRAIN {class_name}\"):\n",
        "            dest_path = os.path.join(TRAINING_DIR, class_name, os.path.basename(img_path))\n",
        "            try:\n",
        "                shutil.copy(img_path, dest_path)\n",
        "                split_results[class_name]['train_sukses'] += 1\n",
        "                total_files_moved += 1\n",
        "            except Exception as e:\n",
        "                split_results[class_name]['gagal_copy'] += 1\n",
        "                print(f\"\\n‚ùå GAGAL COPY file {os.path.basename(img_path)} ke TRAIN: {e}\")\n",
        "\n",
        "        # Menyalin gambar ke folder Validation\n",
        "        for img_path in tqdm(val_images, desc=f\"VAL {class_name}\"):\n",
        "            dest_path = os.path.join(VALIDATION_DIR, class_name, os.path.basename(img_path))\n",
        "            try:\n",
        "                shutil.copy(img_path, dest_path)\n",
        "                split_results[class_name]['val_sukses'] += 1\n",
        "                total_files_moved += 1\n",
        "            except Exception as e:\n",
        "                split_results[class_name]['gagal_copy'] += 1\n",
        "                print(f\"\\n‚ùå GAGAL COPY file {os.path.basename(img_path)} ke VAL: {e}\")\n",
        "\n",
        "    # --- 3. Laporan Akhir dan Diagnostik ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    if total_files_moved > 0:\n",
        "        print(f\"‚úÖ Pembagian Dataset Selesai. Total {total_files_moved} file berhasil diproses.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Pembagian Gagal: Tidak ada file gambar yang berhasil disalin.\")\n",
        "\n",
        "    print(\"\\n### DIAGNOSTIK AKURASI SPLIT ###\")\n",
        "    print(f\"{'Kelas':<25} | {'Total Mentah':<12} | {'Train Sukses':<12} | {'Val Sukses':<10} | {'Gagal Copy':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for class_name, res in sorted(split_results.items()):\n",
        "        total_split = res['train_sukses'] + res['val_sukses'] + res['gagal_copy']\n",
        "\n",
        "        if res['mentah'] > 0:\n",
        "            if total_split != res['mentah']:\n",
        "                print(f\"‚ö†Ô∏è {class_name:<23} | {res['mentah']:<12} | {res['train_sukses']:<12} | {res['val_sukses']:<10} | {res['gagal_copy']:<10} | SELISIH: {res['mentah'] - total_split}\")\n",
        "            else:\n",
        "                 print(f\"‚úÖ {class_name:<23} | {res['mentah']:<12} | {res['train_sukses']:<12} | {res['val_sukses']:<10} | {res['gagal_copy']:<10}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi error fatal saat pembagian data: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9klCPYYQUWMU"
      },
      "outputs": [],
      "source": [
        "# --- HTIUNG TOTAL DATASET MENTAH & DATASET SPLIT (UNTUK COMPARE DATASET MENTAH DAN SPLIT) ---\n",
        "\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "# --- KONFIGURASI PATH UTAMA ---\n",
        "DATASET_MENTAH_DIR = \"/content/drive/MyDrive/DATASET_CNN_XPLORIN/Xplorin\"\n",
        "BASE_SPLIT_DIR = \"/content/drive/MyDrive/DATASET_CNN_XPLORIN/dataset_split\"\n",
        "TRAINING_DIR = os.path.join(BASE_SPLIT_DIR, \"training\")\n",
        "VALIDATION_DIR = os.path.join(BASE_SPLIT_DIR, \"validation\")\n",
        "\n",
        "IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.JPG', '.JPEG', '.PNG')\n",
        "\n",
        "def count_images_in_directory(dataset_path):\n",
        "    \"\"\"Menghitung total file gambar per subfolder (kelas) dalam satu direktori.\"\"\"\n",
        "    class_counts = defaultdict(int)\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        return None\n",
        "\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        class_name = os.path.basename(root)\n",
        "\n",
        "        # Hitung file gambar di folder saat ini\n",
        "        for filename in files:\n",
        "            if filename.lower().endswith(IMAGE_EXTENSIONS):\n",
        "                if root != dataset_path:\n",
        "                    class_counts[class_name] += 1\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "def compare_split_integrity():\n",
        "    \"\"\"Mengumpulkan hitungan dari dataset mentah, training, dan validation, lalu membandingkannya.\"\"\"\n",
        "\n",
        "    print(\"--- MEMULAI INTEGRITAS & VERIFIKASI SPLIT ---\")\n",
        "\n",
        "    # 1. Hitung Dataset Mentah\n",
        "    raw_counts = count_images_in_directory(DATASET_MENTAH_DIR)\n",
        "\n",
        "    if raw_counts is None:\n",
        "        print(f\"‚ùå ERROR: Direktori Mentah ({DATASET_MENTAH_DIR}) tidak ditemukan. Periksa path sumber Anda.\")\n",
        "        return\n",
        "\n",
        "    # 2. Hitung Dataset Training\n",
        "    train_counts = count_images_in_directory(TRAINING_DIR)\n",
        "\n",
        "    # 3. Hitung Dataset Validation\n",
        "    val_counts = count_images_in_directory(VALIDATION_DIR)\n",
        "\n",
        "    if train_counts is None or val_counts is None:\n",
        "        print(f\"‚ùå ERROR: Direktori Split (Train/Val) tidak ditemukan di {BASE_SPLIT_DIR}. Jalankan skrip split data terlebih dahulu!\")\n",
        "        return\n",
        "\n",
        "    # Gabungkan semua nama kelas dari semua direktori\n",
        "    all_classes = sorted(set(raw_counts.keys()) | set(train_counts.keys()) | set(val_counts.keys()))\n",
        "\n",
        "    # --- 4. Buat Tabel Perbandingan ---\n",
        "    table_data = []\n",
        "    global_raw_total = 0\n",
        "    global_split_total = 0\n",
        "\n",
        "    for class_name in all_classes:\n",
        "        raw = raw_counts.get(class_name, 0)\n",
        "        train = train_counts.get(class_name, 0)\n",
        "        val = val_counts.get(class_name, 0)\n",
        "\n",
        "        split_total = train + val\n",
        "        difference = raw - split_total\n",
        "\n",
        "        global_raw_total += raw\n",
        "        global_split_total += split_total\n",
        "\n",
        "        # Penentuan status untuk visualisasi mudah\n",
        "        status = \"‚úÖ OK\" if difference == 0 else (\n",
        "                 \"‚ö†Ô∏è SELISIH\" if difference > 0 else \"‚ùå OVERFLOW\")\n",
        "\n",
        "        table_data.append([\n",
        "            class_name,\n",
        "            raw,\n",
        "            train,\n",
        "            val,\n",
        "            split_total,\n",
        "            difference,\n",
        "            status\n",
        "        ])\n",
        "\n",
        "    # Baris Total\n",
        "    table_data.append([\"---\", \"---\", \"---\", \"---\", \"---\", \"---\", \"---\"])\n",
        "    table_data.append([\n",
        "        \"TOTAL\",\n",
        "        global_raw_total,\n",
        "        sum(train_counts.values()),\n",
        "        sum(val_counts.values()),\n",
        "        global_split_total,\n",
        "        global_raw_total - global_split_total,\n",
        "        \"‚ùå FAIL\" if global_raw_total - global_split_total != 0 else \"‚úÖ OK\"\n",
        "    ])\n",
        "\n",
        "    # Tampilkan Tabel\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"### üìä HASIL INTEGRITAS SPLIT DATASET ###\")\n",
        "    print(tabulate(table_data,\n",
        "                   headers=[\"Kelas\", \"Total Mentah\", \"Train Count\", \"Val Count\", \"Split Total\", \"Selisih\", \"Status\"],\n",
        "                   tablefmt=\"fancy_grid\",\n",
        "                   numalign=\"center\"))\n",
        "    print(\"=\"*100)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        compare_split_integrity()\n",
        "    except NameError:\n",
        "        print(\"‚ùå ERROR: Library 'tabulate' tidak ditemukan. Jalankan: !pip install tabulate\")\n",
        "    except Exception as e:\n",
        "        print(f\"Terjadi error saat menjalankan komparasi: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7LoLuYhi-PJw"
      ],
      "mount_file_id": "1kYwBPGjHxVfQUWs-q7TVK8QczfoRX8wP",
      "authorship_tag": "ABX9TyNL91eY3ApQmzxvM01Yl4Dq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}